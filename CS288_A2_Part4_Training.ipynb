{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff505525",
   "metadata": {},
   "source": [
    "# CS288 Assignment 2 - Part 4: Training & Evaluation\n",
    "\n",
    "## Complete Pipeline for Part 4A & 4B\n",
    "\n",
    "This notebook will guide you through:\n",
    "- **Part 4A**: Pre-training a language model and fine-tuning for multiple-choice QA\n",
    "- **Part 4B**: Using prompting to solve MCQA\n",
    "\n",
    "### Required Deliverables:\n",
    "- `finetuned_predictions.json` (Part 4A)\n",
    "- `prompting_predictions.json` (Part 4B)\n",
    "\n",
    "### Instructions:\n",
    "1. Upload your `cs288-sp26-a2.zip` to Google Drive (see Cell 2)\n",
    "2. Run all cells in order\n",
    "3. Download the generated JSON files at the end\n",
    "\n",
    "---\n",
    "\n",
    "**Estimated Runtime:**\n",
    "- Quick config: ~5-10 minutes\n",
    "- Small config: ~15-30 minutes  \n",
    "- Medium config: ~1-2 hours\n",
    "\n",
    "**GPU Recommendation:** Enable GPU via `Runtime ‚Üí Change runtime type ‚Üí GPU (T4)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d55d788",
   "metadata": {},
   "source": [
    "## 1. Setup: Mount Google Drive and Extract Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848980c6",
   "metadata": {},
   "source": [
    "**Before running this cell:**\n",
    "1. Push your code to GitHub (see instructions below)\n",
    "2. Make your repo public OR generate a personal access token for private repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a3ef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# CHANGE THIS to your GitHub username and repo name\n",
    "GITHUB_USERNAME = \"shanayamalik\"\n",
    "REPO_NAME = \"cs288-sp26-a2\"\n",
    "\n",
    "# Clone your repository\n",
    "!git clone https://github.com/{GITHUB_USERNAME}/{REPO_NAME}.git /content/cs288-sp26-a2\n",
    "\n",
    "# Change to project directory\n",
    "%cd /content/cs288-sp26-a2\n",
    "\n",
    "print(\"‚úÖ Repository cloned successfully!\")\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863e0c66",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514b4e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch tiktoken datasets\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d644e379",
   "metadata": {},
   "source": [
    "## 3. Test Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69552453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/content/cs288-sp26-a2')\n",
    "\n",
    "# Test Part 1 imports\n",
    "from part1.tokenizer import get_tokenizer\n",
    "from part1.train_bpe import train_bpe\n",
    "\n",
    "# Test Part 2 imports\n",
    "from part2.model import TransformerLM\n",
    "\n",
    "# Test Part 3 imports\n",
    "from part3.nn_utils import cross_entropy, gradient_clipping, token_accuracy, perplexity\n",
    "\n",
    "# Test Part 4 imports\n",
    "from part4.datasets import create_pretraining_dataloader, create_qa_dataloader\n",
    "from part4.trainer import Trainer, TrainingConfig\n",
    "from part4.sampling import generate_text, greedy_decode, top_k_decode\n",
    "from part4.qa_model import TransformerForMultipleChoice, evaluate_qa_model\n",
    "from part4.prompting import PromptTemplate, PromptingPipeline, evaluate_prompting\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2475074",
   "metadata": {},
   "source": [
    "## 4. Configuration\n",
    "\n",
    "**Adjust these settings based on your compute budget:**\n",
    "\n",
    "- **quick**: Fast testing (~5-10 min) - Small model, small data\n",
    "- **small**: Basic training (~15-30 min) - 10k stories, ~10M params\n",
    "- **medium**: Better quality (~1-2 hours) - 10k stories, ~50M params\n",
    "\n",
    "For submission, recommend using at least **small** configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e028d263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Choose configuration: \"quick\", \"small\", or \"medium\"\n",
    "CONFIG_NAME = \"small\"  # <-- CHANGE THIS\n",
    "\n",
    "CONFIGS = {\n",
    "    \"quick\": {\n",
    "        \"vocab_size\": 512,\n",
    "        \"d_model\": 128,\n",
    "        \"num_layers\": 4,\n",
    "        \"num_heads\": 4,\n",
    "        \"d_ff\": 512,\n",
    "        \"context_length\": 256,\n",
    "        \"pretrain_epochs\": 2,\n",
    "        \"finetune_epochs\": 5,\n",
    "        \"batch_size\": 32,\n",
    "        \"lr\": 1e-3,\n",
    "        \"num_stories\": 1000,  # For TinyStories download\n",
    "    },\n",
    "    \"small\": {\n",
    "        \"vocab_size\": 4096,\n",
    "        \"d_model\": 256,\n",
    "        \"num_layers\": 6,\n",
    "        \"num_heads\": 8,\n",
    "        \"d_ff\": 1024,\n",
    "        \"context_length\": 512,\n",
    "        \"pretrain_epochs\": 3,\n",
    "        \"finetune_epochs\": 10,\n",
    "        \"batch_size\": 32,\n",
    "        \"lr\": 3e-4,\n",
    "        \"num_stories\": 10000,\n",
    "    },\n",
    "    \"medium\": {\n",
    "        \"vocab_size\": 8192,\n",
    "        \"d_model\": 512,\n",
    "        \"num_layers\": 8,\n",
    "        \"num_heads\": 8,\n",
    "        \"d_ff\": 2048,\n",
    "        \"context_length\": 512,\n",
    "        \"pretrain_epochs\": 5,\n",
    "        \"finetune_epochs\": 15,\n",
    "        \"batch_size\": 16,\n",
    "        \"lr\": 1e-4,\n",
    "        \"num_stories\": 10000,\n",
    "    }\n",
    "}\n",
    "\n",
    "CONFIG = CONFIGS[CONFIG_NAME]\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using configuration: {CONFIG_NAME}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Model will have ~{CONFIG['d_model']**2 * CONFIG['num_layers'] * 12 / 1e6:.1f}M parameters\")\n",
    "print(f\"\\nConfig details:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ca0a6d",
   "metadata": {},
   "source": [
    "## 5. Download TinyStories Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f987d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "os.makedirs('/content/cs288-sp26-a2/part4/fixtures', exist_ok=True)\n",
    "\n",
    "print(\"Downloading TinyStories dataset...\")\n",
    "num_stories = CONFIG[\"num_stories\"]\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\", split=f\"train[:{num_stories}]\")\n",
    "print(f\"‚úÖ Loaded {len(dataset):,} stories\")\n",
    "\n",
    "# Save to text file\n",
    "PRETRAIN_DATA = Path('/content/cs288-sp26-a2/part4/fixtures/tinystories_train.txt')\n",
    "with open(PRETRAIN_DATA, 'w', encoding='utf-8') as f:\n",
    "    for i, story in enumerate(dataset):\n",
    "        f.write(story['text'])\n",
    "        f.write('\\n<|endoftext|>\\n')\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(f\"  Processed {i+1}/{len(dataset)} stories...\")\n",
    "\n",
    "file_size_mb = os.path.getsize(PRETRAIN_DATA) / 1024 / 1024\n",
    "print(f\"‚úÖ Saved to {PRETRAIN_DATA}\")\n",
    "print(f\"   File size: {file_size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c394dcc",
   "metadata": {},
   "source": [
    "## 6. Train BPE Tokenizer (Part 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ededbcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: Training BPE Tokenizer\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "SPECIAL_TOKENS = [\"<|endoftext|>\", \"<|pad|>\"]\n",
    "\n",
    "print(f\"Training on: {PRETRAIN_DATA}\")\n",
    "print(f\"Target vocab size: {CONFIG['vocab_size']}\")\n",
    "print(f\"Special tokens: {SPECIAL_TOKENS}\")\n",
    "print()\n",
    "\n",
    "vocab, merges = train_bpe(\n",
    "    input_path=PRETRAIN_DATA,\n",
    "    vocab_size=CONFIG['vocab_size'],\n",
    "    special_tokens=SPECIAL_TOKENS,\n",
    ")\n",
    "\n",
    "tokenizer = get_tokenizer(vocab, merges, SPECIAL_TOKENS)\n",
    "\n",
    "print(f\"\\n‚úÖ Tokenizer trained!\")\n",
    "print(f\"   Vocab size: {len(vocab)}\")\n",
    "print(f\"   Num merges: {len(merges)}\")\n",
    "\n",
    "# Test encoding\n",
    "test_text = \"Once upon a time, there was a little girl.\"\n",
    "tokens = tokenizer.encode(test_text)\n",
    "decoded = tokenizer.decode(tokens)\n",
    "\n",
    "print(f\"\\nüìù Test encoding:\")\n",
    "print(f\"   Input:   '{test_text}'\")\n",
    "print(f\"   Tokens:  {len(tokens)} tokens ‚Üí {tokens[:10]}...\")\n",
    "print(f\"   Decoded: '{decoded}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e565d2a",
   "metadata": {},
   "source": [
    "## 7. Create TransformerLM Model (Part 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155e413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"STEP 2: Creating Transformer Language Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model = TransformerLM(\n",
    "    vocab_size=len(tokenizer.vocab),\n",
    "    context_length=CONFIG[\"context_length\"],\n",
    "    d_model=CONFIG[\"d_model\"],\n",
    "    num_layers=CONFIG[\"num_layers\"],\n",
    "    num_heads=CONFIG[\"num_heads\"],\n",
    "    d_ff=CONFIG[\"d_ff\"],\n",
    ").to(DEVICE)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "model_size_mb = num_params * 4 / 1024 / 1024  # fp32\n",
    "\n",
    "print(f\"\\n‚úÖ Model created!\")\n",
    "print(f\"\\nüìä Model Architecture:\")\n",
    "print(f\"   Vocab size: {len(tokenizer.vocab):,}\")\n",
    "print(f\"   Context length: {CONFIG['context_length']}\")\n",
    "print(f\"   d_model: {CONFIG['d_model']}\")\n",
    "print(f\"   Layers: {CONFIG['num_layers']}\")\n",
    "print(f\"   Attention heads: {CONFIG['num_heads']}\")\n",
    "print(f\"   FFN dimension: {CONFIG['d_ff']}\")\n",
    "print(f\"\\nüíæ Model Size:\")\n",
    "print(f\"   Parameters: {num_params:,}\")\n",
    "print(f\"   Memory (fp32): ~{model_size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bac7f0",
   "metadata": {},
   "source": [
    "## 8. Pre-train Model (Language Modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c8d0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"STEP 3: Pre-training Language Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create dataloader\n",
    "train_dataloader = create_pretraining_dataloader(\n",
    "    file_path=PRETRAIN_DATA,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    max_length=CONFIG[\"context_length\"],\n",
    "    stride=CONFIG[\"context_length\"] // 2,  # 50% overlap\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nüìö Training Data:\")\n",
    "print(f\"   Sequences: {len(train_dataloader.dataset):,}\")\n",
    "print(f\"   Batches/epoch: {len(train_dataloader)}\")\n",
    "print(f\"   Tokens/epoch: ~{len(train_dataloader) * CONFIG['batch_size'] * CONFIG['context_length']:,}\")\n",
    "\n",
    "# Training configuration\n",
    "train_config = TrainingConfig(\n",
    "    num_epochs=CONFIG[\"pretrain_epochs\"],\n",
    "    learning_rate=CONFIG[\"lr\"],\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=min(100, len(train_dataloader) // 5),\n",
    "    max_grad_norm=1.0,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    device=DEVICE,\n",
    "    log_interval=max(1, len(train_dataloader) // 5),\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    config=train_config,\n",
    "    train_dataloader=train_dataloader,\n",
    ")\n",
    "\n",
    "print(f\"\\nüöÄ Training Configuration:\")\n",
    "print(f\"   Epochs: {train_config.num_epochs}\")\n",
    "print(f\"   Learning rate: {train_config.learning_rate}\")\n",
    "print(f\"   Warmup steps: {train_config.warmup_steps}\")\n",
    "print(f\"   Batch size: {train_config.batch_size}\")\n",
    "print(f\"   Device: {DEVICE}\")\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = trainer.train()\n",
    "\n",
    "print(f\"\\n‚úÖ Pre-training complete!\")\n",
    "print(f\"   Final training loss: {trainer.train_losses[-1]:.4f}\")\n",
    "print(f\"   Training losses: {[f'{loss:.4f}' for loss in trainer.train_losses]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794c1632",
   "metadata": {},
   "source": [
    "## 9. Test Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fbe5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TEXT GENERATION SAMPLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "prompts = [\n",
    "    \"Once upon a time\",\n",
    "    \"The little dog\",\n",
    "    \"There was a princess\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nüìù Prompt: '{prompt}'\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Greedy decoding\n",
    "    greedy_text = generate_text(\n",
    "        model, tokenizer, prompt,\n",
    "        max_new_tokens=50,\n",
    "        method=\"greedy\"\n",
    "    )\n",
    "    print(f\"Greedy: {greedy_text}\")\n",
    "    \n",
    "    # Top-k sampling\n",
    "    topk_text = generate_text(\n",
    "        model, tokenizer, prompt,\n",
    "        max_new_tokens=50,\n",
    "        method=\"top_k\",\n",
    "        k=50,\n",
    "        temperature=0.8\n",
    "    )\n",
    "    print(f\"Top-k:  {topk_text}\")\n",
    "\n",
    "print(\"\\n‚úÖ Generation test complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad84b37",
   "metadata": {},
   "source": [
    "## 10. Fine-tune for Multiple-Choice QA (Part 4A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc963848",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"STEP 4: Fine-tuning for Multiple-Choice QA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create MCQA model with classification head\n",
    "qa_model = TransformerForMultipleChoice(\n",
    "    transformer_lm=model,\n",
    "    hidden_size=CONFIG[\"d_model\"],\n",
    "    num_choices=4,\n",
    "    pooling=\"last\",\n",
    "    freeze_backbone=False,  # Allow fine-tuning of base model\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"‚úÖ QA Model created with classification head\")\n",
    "\n",
    "# Load QA datasets\n",
    "QA_TRAIN = Path('/content/cs288-sp26-a2/part4/fixtures/squad_train.json')\n",
    "QA_DEV = Path('/content/cs288-sp26-a2/part4/fixtures/squad_dev.json')\n",
    "\n",
    "qa_train_loader = create_qa_dataloader(\n",
    "    data=QA_TRAIN,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    max_length=CONFIG[\"context_length\"],\n",
    "    num_choices=4,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "qa_dev_loader = create_qa_dataloader(\n",
    "    data=QA_DEV,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    max_length=CONFIG[\"context_length\"],\n",
    "    num_choices=4,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "print(f\"\\nüìö QA Data:\")\n",
    "print(f\"   Training examples: {len(qa_train_loader.dataset)}\")\n",
    "print(f\"   Dev examples: {len(qa_dev_loader.dataset)}\")\n",
    "\n",
    "# Custom loss function for QA\n",
    "def qa_loss_fn(batch, model):\n",
    "    input_ids = batch[\"input_ids\"].to(DEVICE)  # [batch, num_choices, seq_len]\n",
    "    attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "    labels = batch[\"labels\"].to(DEVICE)  # [batch]\n",
    "    \n",
    "    logits = model(input_ids, attention_mask)  # [batch, num_choices]\n",
    "    return cross_entropy(logits, labels)\n",
    "\n",
    "# Fine-tuning configuration\n",
    "finetune_config = TrainingConfig(\n",
    "    num_epochs=CONFIG[\"finetune_epochs\"],\n",
    "    learning_rate=CONFIG[\"lr\"] / 2,  # Lower LR for fine-tuning\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=min(50, len(qa_train_loader) // 5),\n",
    "    max_grad_norm=1.0,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    device=DEVICE,\n",
    "    log_interval=max(1, len(qa_train_loader) // 5),\n",
    ")\n",
    "\n",
    "qa_trainer = Trainer(\n",
    "    model=qa_model,\n",
    "    config=finetune_config,\n",
    "    train_dataloader=qa_train_loader,\n",
    "    val_dataloader=qa_dev_loader,\n",
    "    compute_loss_fn=qa_loss_fn,\n",
    ")\n",
    "\n",
    "print(f\"\\nüöÄ Fine-tuning Configuration:\")\n",
    "print(f\"   Epochs: {finetune_config.num_epochs}\")\n",
    "print(f\"   Learning rate: {finetune_config.learning_rate}\")\n",
    "print(f\"   Warmup steps: {finetune_config.warmup_steps}\")\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"Starting fine-tuning...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "finetune_results = qa_trainer.train()\n",
    "\n",
    "print(f\"\\n‚úÖ Fine-tuning complete!\")\n",
    "if qa_trainer.val_losses:\n",
    "    print(f\"   Final validation loss: {qa_trainer.val_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f7e0b4",
   "metadata": {},
   "source": [
    "## 11. Generate Fine-tuned Predictions (Part 4A Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6997183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"STEP 5: Evaluating Fine-tuned Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Evaluate on dev set\n",
    "accuracy = evaluate_qa_model(qa_model, qa_dev_loader, DEVICE)\n",
    "print(f\"\\nüìä Fine-tuned Model Performance:\")\n",
    "print(f\"   Dev Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Generate predictions on test set\n",
    "QA_TEST = Path('/content/cs288-sp26-a2/part4/fixtures/squad_test.json')\n",
    "qa_test_loader = create_qa_dataloader(\n",
    "    data=QA_TEST,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    max_length=CONFIG[\"context_length\"],\n",
    "    num_choices=4,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "print(f\"\\nüîç Generating predictions on test set ({len(qa_test_loader.dataset)} examples)...\")\n",
    "\n",
    "predictions = []\n",
    "qa_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in qa_test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        \n",
    "        logits = qa_model(input_ids, attention_mask)\n",
    "        preds = logits.argmax(dim=-1).cpu().tolist()\n",
    "        predictions.extend(preds)\n",
    "\n",
    "# Save predictions\n",
    "import json\n",
    "\n",
    "os.makedirs('/content/outputs', exist_ok=True)\n",
    "output_file = '/content/outputs/finetuned_predictions.json'\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(predictions, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Predictions saved to: {output_file}\")\n",
    "print(f\"   Total predictions: {len(predictions)}\")\n",
    "print(f\"   Sample predictions: {predictions[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5655ed",
   "metadata": {},
   "source": [
    "## 12. Prompting-Based Evaluation (Part 4B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc82cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"STEP 6: Prompting-Based Evaluation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create prompt template\n",
    "prompt_template = PromptTemplate()\n",
    "\n",
    "# Create prompting pipeline using the base LM (not the QA model)\n",
    "prompting_pipeline = PromptingPipeline(\n",
    "    model=model,  # Use the base TransformerLM\n",
    "    tokenizer=tokenizer,\n",
    "    prompt_template=prompt_template,\n",
    "    max_length=CONFIG[\"context_length\"],\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Prompting pipeline created\")\n",
    "\n",
    "# Evaluate on dev set\n",
    "print(f\"\\nüîç Evaluating prompting on dev set...\")\n",
    "prompting_accuracy = evaluate_prompting(\n",
    "    prompting_pipeline,\n",
    "    qa_dev_loader,\n",
    "    DEVICE\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Prompting Model Performance:\")\n",
    "print(f\"   Dev Accuracy: {prompting_accuracy:.2%}\")\n",
    "print(f\"\\nüìà Comparison:\")\n",
    "print(f\"   Fine-tuned: {accuracy:.2%}\")\n",
    "print(f\"   Prompting:  {prompting_accuracy:.2%}\")\n",
    "print(f\"   Difference: {(prompting_accuracy - accuracy):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab2609e",
   "metadata": {},
   "source": [
    "## 13. Generate Prompting Predictions (Part 4B Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5ac249",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Generating prompting predictions on test set...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate predictions using prompting\n",
    "prompting_predictions = []\n",
    "\n",
    "# Load test data\n",
    "with open(QA_TEST, 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"Processing {len(test_data)} test examples...\")\n",
    "\n",
    "model.eval()\n",
    "for i, example in enumerate(test_data):\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"  Processed {i+1}/{len(test_data)} examples...\")\n",
    "    \n",
    "    pred = prompting_pipeline.predict(example)\n",
    "    prompting_predictions.append(pred)\n",
    "\n",
    "# Save predictions\n",
    "prompting_output_file = '/content/outputs/prompting_predictions.json'\n",
    "\n",
    "with open(prompting_output_file, 'w') as f:\n",
    "    json.dump(prompting_predictions, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Prompting predictions saved to: {prompting_output_file}\")\n",
    "print(f\"   Total predictions: {len(prompting_predictions)}\")\n",
    "print(f\"   Sample predictions: {prompting_predictions[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e0b475",
   "metadata": {},
   "source": [
    "## 14. Final Summary & Download Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547baad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüìä Final Results:\")\n",
    "print(f\"   Configuration: {CONFIG_NAME}\")\n",
    "print(f\"   Model parameters: {num_params:,}\")\n",
    "print(f\"   Pre-training epochs: {CONFIG['pretrain_epochs']}\")\n",
    "print(f\"   Fine-tuning epochs: {CONFIG['finetune_epochs']}\")\n",
    "print()\n",
    "print(\"   Fine-tuned model accuracy: {:.2%}\".format(accuracy))\n",
    "print(\"   Prompting model accuracy:  {:.2%}\".format(prompting_accuracy))\n",
    "print(\"   Improvement: {:.2%}\".format(prompting_accuracy - accuracy))\n",
    "print()\n",
    "print(\"üìÅ Output Files:\")\n",
    "print(f\"   ‚úÖ {output_file}\")\n",
    "print(f\"   ‚úÖ {prompting_output_file}\")\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verify files exist\n",
    "assert os.path.exists(output_file), \"Fine-tuned predictions file not found!\"\n",
    "assert os.path.exists(prompting_output_file), \"Prompting predictions file not found!\"\n",
    "\n",
    "# Verify correct format\n",
    "with open(output_file, 'r') as f:\n",
    "    ft_preds = json.load(f)\n",
    "with open(prompting_output_file, 'r') as f:\n",
    "    pr_preds = json.load(f)\n",
    "\n",
    "assert len(ft_preds) == len(test_data), f\"Fine-tuned predictions mismatch: {len(ft_preds)} vs {len(test_data)}\"\n",
    "assert len(pr_preds) == len(test_data), f\"Prompting predictions mismatch: {len(pr_preds)} vs {len(test_data)}\"\n",
    "\n",
    "print(\"‚úÖ All files verified and ready for submission!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3150c2",
   "metadata": {},
   "source": [
    "## 15. Download Submission Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f190aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"Downloading submission files...\")\n",
    "print()\n",
    "\n",
    "# Download both prediction files\n",
    "files.download(output_file)\n",
    "print(f\"‚úÖ Downloaded: finetuned_predictions.json\")\n",
    "\n",
    "files.download(prompting_output_file)\n",
    "print(f\"‚úÖ Downloaded: prompting_predictions.json\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ ALL DONE!\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"Next steps:\")\n",
    "print(\"1. Check your Downloads folder for the JSON files\")\n",
    "print(\"2. Submit both files according to assignment instructions\")\n",
    "print()\n",
    "print(\"Expected grading:\")\n",
    "print(f\"  - Fine-tuned accuracy: {accuracy:.2%}\")\n",
    "print(f\"  - Prompting accuracy: {prompting_accuracy:.2%}\")\n",
    "print()\n",
    "print(\"Good luck! üöÄ\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
